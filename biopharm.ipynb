{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import StratifiedKFold, train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import (\n    Embedding, \n    Conv1D, \n    GlobalMaxPooling1D, \n    Dense, \n    Dropout, \n    SpatialDropout1D, \n    GRU, \n    LSTM, \n    Bidirectional, \n    SimpleRNN, \n    MaxPooling1D, \n    Flatten, \n    BatchNormalization, \n    SeparableConv1D\n)\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping\nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import WordNetLemmatizer\nimport pandas as pd\nfrom sklearn.model_selection import StratifiedKFold, train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import (\n    Embedding, \n    Conv1D, \n    GlobalMaxPooling1D, \n    Dense, \n    Dropout, \n    SpatialDropout1D, \n    GRU, \n    LSTM, \n    Bidirectional, \n    SimpleRNN, \n    MaxPooling1D, \n    Flatten, \n    BatchNormalization, \n    SeparableConv1D\n)\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping\nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import WordNetLemmatizer\nimport pandas as pd\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.preprocessing import LabelEncoder\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras import Input, Model\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import WordNetLemmatizer\nimport nltk\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom keras.models import Sequential\nfrom keras.layers import (\n    Embedding, \n    Conv1D, \n    GlobalMaxPooling1D, \n    Dense, \n    Dropout, \n    SpatialDropout1D, \n    GRU, \n    LSTM, \n    Bidirectional, \n    SimpleRNN, \n    MaxPooling1D, \n    Flatten, \n    BatchNormalization, \n    GlobalAveragePooling1D, \n    SeparableConv1D, \n    Attention,\n    Concatenate\n)\nfrom keras.optimizers import Adam\nfrom keras.callbacks import EarlyStopping, ReduceLROnPlateau\nfrom keras.regularizers import l2\nfrom keras.optimizers.schedules import ExponentialDecay\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom keras.preprocessing.sequence import pad_sequences\n\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data=pd.read_csv(\"/kaggle/input/bestofbest/best_data.csv\")\ntest_data=pd.read_csv(\"/kaggle/input/new-dataf/HTS.csv\")\ndel test_data['Unnamed: 0']\ndel test_data['Index']\n# del train_data['Unnamed: 0']\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.dropna(subset=['Event.type'], inplace=True)\n\n# Reset index after dropping rows\ntrain_data.reset_index(drop=True, inplace=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data = train_data[train_data['Event.type'] != '-']\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# Assuming train_data and test_data are pandas DataFrame objects\n\n# Combining columns into a new 'combined' column\ntrain_data['combined'] = train_data['Index'] + ' ' + train_data['Lieu'] + ' ' + train_data['Nouvelle.valeur'] + ' ' + train_data['Valeur'] + ' ' + train_data['Ancienne.valeur'] + ' ' + train_data['Date']\n# test_data['combined'] = test_data['Index'] + ' ' + test_data['Lieu'] + ' ' + test_data['Nouvelle.valeur'] + ' ' + test_data['Valeur'] + ' ' + test_data['Ancienne.valeur'] + ' ' + test_data['Date']\n\n# Dropping unnecessary columns\ntrain_data = train_data.drop(columns=['Index', 'Lieu', 'Valeur', 'Nouvelle.valeur', 'Ancienne.valeur'])\n# test_data = test_data.drop(columns=['Index', 'Lieu', 'Valeur', 'Nouvelle.valeur', 'Ancienne.valeur'])\ndel train_data['Unnamed: 0']\ndel train_data['Date']","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import StratifiedKFold\nfrom sklearn.preprocessing import LabelEncoder\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Embedding, SpatialDropout1D, BatchNormalization, Conv1D, MaxPooling1D, SeparableConv1D, Bidirectional, LSTM, GRU, SimpleRNN, Dropout, Dense, GlobalMaxPooling1D, Flatten\nfrom keras.optimizers import RMSprop\nfrom keras.callbacks import EarlyStopping\nfrom keras.regularizers import l2\nimport pandas as pd\nimport numpy as np\n\n# Load the data\n# df_train = pd.read_csv('/kaggle/input/data-best/NLPT.csv')\n# train_data = df_train.drop_duplicates(subset=\"combined\").reset_index(drop=True)\n# test_data = pd.read_csv('/kaggle/input/data-best/NLPS.csv')\n\n# Preprocess the data\nX = train_data['combined']\ny = train_data['Event.type']\n\n# Encode target labels into integers\nlabel_encoder = LabelEncoder()\ny_encoded = label_encoder.fit_transform(y)\n\n# Tokenize and pad sequences\nmax_words = 100\nmax_len = 100\n\ntokenizer = Tokenizer(num_words=max_words)\ntokenizer.fit_on_texts(X)\n\nX_seq = tokenizer.texts_to_sequences(X)\nX_pad = pad_sequences(X_seq, maxlen=max_len)\n\n# Define the CNN model\ndef create_model():\n    model = Sequential(name=\"Text_Classification_Model\")\n    model.add(Embedding(input_dim=max_words, output_dim=300, input_length=max_len, name=\"embedding_layer\"))\n    model.add(SpatialDropout1D(0.4, name=\"spatial_dropout\"))\n    model.add(BatchNormalization(name=\"batch_norm_\"))\n    model.add(Conv1D(128, 3, activation='relu', kernel_regularizer=l2(1e-4), padding='same', name=\"conv1d_1\"))\n    model.add(BatchNormalization(name=\"batch_norm_1\"))\n    model.add(Conv1D(64, 3, activation='relu', kernel_regularizer=l2(1e-4), padding='same', name=\"conv1d_2\"))\n    model.add(BatchNormalization(name=\"batch_norm_2\"))\n    model.add(Conv1D(64, 3, activation='relu', kernel_regularizer=l2(1e-4), padding='same', name=\"conv1d_3\"))\n    model.add(MaxPooling1D(2, name=\"max_pooling\"))\n    model.add(SeparableConv1D(64, 3, activation='relu', kernel_regularizer=l2(1e-4), padding='same', name=\"separable_conv1d\"))\n    model.add(Bidirectional(LSTM(128, return_sequences=True), name=\"bidirectional_lstm\"))\n    model.add(LSTM(128, return_sequences=True, name=\"cudnn_lstm\"))\n    model.add(Bidirectional(GRU(128, return_sequences=True), name=\"bidirectional_gru\"))\n    model.add(GRU(128, return_sequences=True, name=\"cudnn_gru\"))\n    model.add(SimpleRNN(32, return_sequences=True, name=\"simple_rnn\"))\n    model.add(Bidirectional(GRU(128, return_sequences=True), name=\"bidirectnal_gru\"))\n    model.add(GRU(128, return_sequences=True, name=\"cnn_gru\"))\n    model.add(SimpleRNN(32, return_sequences=True, name=\"smple_rnn\"))\n    model.add(Dense(128, activation='relu', kernel_regularizer=l2(1e-4), name=\"dense_1\"))\n    model.add(BatchNormalization(name=\"batch_norm_3\"))\n    model.add(Dropout(0.1, name=\"dropout\"))\n    model.add(Dense(64, activation='relu', kernel_regularizer=l2(1e-4), name=\"dense_2\"))\n    model.add(Dropout(0.2, name=\"dropout_2\"))\n    model.add(GlobalMaxPooling1D(name=\"global_max_pooling\"))\n    model.add(Flatten(name=\"flatten\"))\n    num_classes = 14\n    model.add(Dense(num_classes, activation='softmax', name=\"output_layer\"))\n    optimizer = RMSprop(learning_rate=0.0001)\n    model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy')\n    return model\n\n# Define EarlyStopping callback\nearly_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n\n# Define cross-validation\nkfold = StratifiedKFold(n_splits=6, shuffle=True, random_state=42)\n\n# Initialize variables for storing results\ncv_scores = []\nfold = 1\n\n# Create and compile the model\nmodel = create_model()\nmodel.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n\n# Perform cross-validation\nfor train_idx, val_idx in kfold.split(X_pad, y_encoded):\n    print(f\"Fold {fold}\")\n\n    # Split data into training and validation sets\n    X_train, X_val = X_pad[train_idx], X_pad[val_idx]\n    y_train, y_val = y_encoded[train_idx], y_encoded[val_idx]\n\n    # Train the model\n    history = model.fit(X_train, y_train, epochs=7, batch_size=256, callbacks=[early_stopping], validation_data=(X_val, y_val))\n\n    # Evaluate the model on validation data\n    accuracy = history.history['val_accuracy'][-1]\n    cv_scores.append(accuracy)\n\n    fold += 1\n\n# Print the cross-validation scores\nprint(\"Cross-validation scores:\", cv_scores)\nprint(\"Mean accuracy:\", np.mean(cv_scores))\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}